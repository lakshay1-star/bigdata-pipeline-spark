# Big Data Pipeline with PySpark

This project demonstrates a scalable, production-ready big data pipeline using **Apache Spark (PySpark)**. It forecasts container volume for a port terminal using a synthetic dataset, showcasing best practices in configuration management, modular code, logging, and testing.

## üöÄ Features
- **Config-Driven:** Pipeline parameters are managed via a central `config.yaml` file.
- **Modular & Scalable:** Code is organized into logical modules for data preprocessing, model training, and evaluation.
- **Executable Script:** The entire pipeline can be run from the command line, making it suitable for automation and scheduling.
- **Logging:** Robust logging for monitoring and debugging pipeline execution.
- **Unit Tested:** Core components are verified with unit tests using `pytest`.
- **Machine Learning:** Uses a `RandomForestRegressor` for container volume forecasting.
- **Evaluation:** Model performance is measured with Root Mean Squared Error (RMSE) and R-squared ($R^2$) metrics.

<!-- ## üìÇ Project Structure
bigdata-pipeline-spark/
‚îÇ‚îÄ‚îÄ README.md
‚îÇ‚îÄ‚îÄ requirements.txt
‚îÇ‚îÄ‚îÄ main.py                 # Main executable script
‚îÇ‚îÄ‚îÄ config.yaml             # Configuration file
‚îÇ‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_analysis.ipynb
‚îÇ‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py
‚îÇ‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ container_data.csv
‚îÇ‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ metrics.txt
‚îÇ‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.log
‚îî‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ test_data_preprocessing.py -->


## ‚ö° Setup

1.  Clone the repository:
    ```bash
    git clone <your-repo-url>
    cd bigdata-pipeline-spark
    ```

2.  Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## ‚ñ∂Ô∏è How to Run the Pipeline

Execute the main script from the root directory of the project. The script will use `config.yaml` to configure the pipeline run.

```bash
python main.py
After execution, you can check the following:

Logs: logs/pipeline.log for detailed execution steps.

Metrics: results/metrics.txt for the final model performance.

üß™ How to Run Tests
To ensure the reliability of the components, run the unit tests using pytest:

Bash

pytest
üìì Exploratory Notebook
An accompanying Jupyter notebook is available for data exploration, prototyping, and visualization. It utilizes the same modular functions from the src/ directory.

To run it:

Bash

jupyter notebook notebooks/pipeline_analysis.ipynb
üìä Expected Results
RMSE: ~450 containers (on synthetic dataset)

R 
2
  Score: ~0.87

Feature importance indicates that container type and arrival volume are the strongest predictors.

üìù Author: Lakshay Naresh Ramchandani