{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Pipeline with PySpark",
    "This notebook demonstrates a PySpark pipeline for predicting container volume at ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession",
    "from src.data_preprocessing import load_data, preprocess_data",
    "from src.model_training import train_model, evaluate_model",
    "from src.evaluation import save_metrics",
    "",
    "spark = SparkSession.builder.appName('BigDataPipeline').getOrCreate()",
    "",
    "df = load_data(spark, '../data/container_data.csv')",
    "df = preprocess_data(df)",
    "model = train_model(df)",
    "rmse, r2 = evaluate_model(model, df)",
    "save_metrics(rmse, r2, '../results/metrics.txt')",
    "print(f'RMSE: {rmse:.2f}, RÂ²: {r2:.2f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
